{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvZzQgzfwI5n",
    "outputId": "9f21f6a6-5e92-4db9-c538-1e4cbc47324e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zz61FuSWelHD",
    "outputId": "fb3e71f2-93b5-4133-bcf7-c16ed767900c"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'find_dotenv' from 'dotenv' (/Users/acucchia/Library/Python/3.10/lib/python/site-packages/dotenv/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_dotenv, load_dotenv\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'find_dotenv' from 'dotenv' (/Users/acucchia/Library/Python/3.10/lib/python/site-packages/dotenv/__init__.py)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urlencode, quote_plus\n",
    "import numpy as np\n",
    "import sys\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "butCJ77F2K_K"
   },
   "source": [
    "# **Citing this code**\n",
    "This code is the second version of a Expertise finding tool developed by Volz et al. 2023 (https://ui.adsabs.harvard.edu/abs/2023AAS...24210207V/abstract).<br>\n",
    "It utilizes NASA ADS API to query for articles (refereed or not) in the \"Astronomy\" database (cite ADS).\n",
    "Please, cite \"Helfenbein et al. 2023 (in prep) and refer to the README file in the github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8Ktffmlx6Qd"
   },
   "source": [
    "**Directory set up**<br>\n",
    "The file *stopwords.txt* is utilized to create meaningful N-grams. Make sure to provide an accurate path in the following cell.<br> Also, the path will be used by the code in other instances to identify other useful files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OiBplztjesQ5"
   },
   "outputs": [],
   "source": [
    "path_stop= '/Your Path/here/",
    "stop_file='stopwords.txt'\n",
    "stop_dir=path_stop+stop_file\n",
    "sys.path.append(path_stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token = 'Your own token from ADS API page ' #Insert your API token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "48rCvxEweuUv"
   },
   "outputs": [],
   "source": [
    "#For the TextAnalysis File, please refer to M. Volze et al. 2023\n",
    "import TextAnalysis as TA\n",
    "import ADSsearcherpkg as AP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE8Y7GfdfCh1"
   },
   "source": [
    "# **Example 1: Searching expertises of a single person based on their name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCHVPlVCgiSR"
   },
   "source": [
    "The search will focus on papers published by a specific author in the past 15 years independently of the current affiliation:<br>\n",
    "The format for a single author search is as follows: **\"Last, First\"**<br>\n",
    "In the following example we search for Dr. Joshua Pepper expertise. <br>\n",
    "**Note:** the user can decide to query ONLY refereed paper adding, before the token keyword the following keyword:<br>\n",
    "**refereed=\"property:refereed\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oD_utgaigqfi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will search for papers matching the following criteria:\n",
      "author:\"^Pepper, Joshua\"\n",
      "\n",
      "I am now querying ADS.\n",
      "\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/acucchia/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m datf\u001b[38;5;241m=\u001b[39m\u001b[43mAP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mads_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPepper, Joshua\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m               \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Interns/IzzyEl/ReviewerExtractor-main 4/codeV3/ADSsearcherpkg.py:136\u001b[0m, in \u001b[0;36mads_search\u001b[0;34m(name, institution, year, refereed, token, stop_dir)\u001b[0m\n\u001b[1;32m    134\u001b[0m     data2 \u001b[38;5;241m=\u001b[39m data_type(df)\n\u001b[1;32m    135\u001b[0m     data3 \u001b[38;5;241m=\u001b[39m merge(data2)\n\u001b[0;32m--> 136\u001b[0m     data4 \u001b[38;5;241m=\u001b[39m \u001b[43mn_grams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data4\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Interns/IzzyEl/ReviewerExtractor-main 4/codeV3/ADSsearcherpkg.py:186\u001b[0m, in \u001b[0;36mn_grams\u001b[0;34m(df, directorypath)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m    185\u001b[0m     abstracts \u001b[38;5;241m=\u001b[39m i[\u001b[38;5;241m8\u001b[39m]\n\u001b[0;32m--> 186\u001b[0m     top10words \u001b[38;5;241m=\u001b[39m \u001b[43mTA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabstracts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectorypath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     top10bigrams \u001b[38;5;241m=\u001b[39m TA\u001b[38;5;241m.\u001b[39mtopbigrams(abstracts, directorypath)\n\u001b[1;32m    188\u001b[0m     top10trigrams \u001b[38;5;241m=\u001b[39m TA\u001b[38;5;241m.\u001b[39mtoptrigrams(abstracts, directorypath)\n",
      "File \u001b[0;32m~/Interns/IzzyEl/ReviewerExtractor-main 4/codeV3/TextAnalysis.py:62\u001b[0m, in \u001b[0;36mtopwords\u001b[0;34m(abstract, directorypath)\u001b[0m\n\u001b[1;32m     59\u001b[0m lower \u001b[38;5;241m=\u001b[39m letters\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Step 4: Tokenize the abstract.\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Step 5: Remove punctuation and stop words.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m punctuation \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m’\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml&gt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml&lt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/acucchia/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "datf=AP.ads_search(name=\"Pepper, Joshua\",\n",
    "               token=token, stop_dir=stop_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "Gb099pC0l0K5",
    "outputId": "dc6002fd-456d-4b40-c063-7e310c65b62d"
   },
   "outputs": [],
   "source": [
    "# To display the data frame run the following:\n",
    "datf\n",
    "# To save it in a excel format run the following:\n",
    "#datf.to_csv(path_stop+\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhYJiM-TgHnk"
   },
   "source": [
    "# **Example 2: Searching expertises of ALL scientists that published as first authors when affiliated to single institution name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzEADBytfPR_"
   },
   "source": [
    "The search will focus on papers and all authors that have published in the past 15 years at a specific institution (academic or otherwise): <br>\n",
    "The format for a single institution is as follows: **institution=\"Institution Name\"**. <br>\n",
    "**Caveat**: It is possible that the institutions as input by the user does not match what has been cataloged in ADS, therefore if the final output is empty, make sure to try different versions of the institution names (e.g. Cal Poly Pomona, Cal Poly, California Polytechnic State University) to get the most complete list of authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-CCxIkmfJfy",
    "outputId": "79dcf782-4cc1-47af-a68a-5c8d1220ba4c"
   },
   "outputs": [],
   "source": [
    "datf=AP.ads_search(institution=\"Hampton University\",refereed=\"property:refereed\",\n",
    "               token=token, stop_dir=stop_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doN5GM4zfVX4"
   },
   "outputs": [],
   "source": [
    "# To display the data frame run the following:\n",
    "datf\n",
    "# To save it in a excel format run the following:\n",
    "#datf.to_csv(path_stop+\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ve-8jI7gJV1"
   },
   "source": [
    "# **Example 3: Searching a single author publication while affiliated to a specific institution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88tt1j3bmZq1"
   },
   "source": [
    "The search will focus on papers published by a single author while they are affiliated to a specific institution, in the past 15 years:<br>\n",
    "\n",
    "The format for a single author and institution is as follows: **name= 'Last, First', institution= 'Institution Name'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H32FoRgemk2v",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datf=AP.ads_search(name= 'Capper, Daniel', institution=\"University of Southern Mississippi\",\n",
    "               token=token, stop_dir=stop_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9WpQ5fnwG5x"
   },
   "outputs": [],
   "source": [
    "# To display the data frame run the following:\n",
    "datf\n",
    "# To save it in a excel format run the following:\n",
    "#datf.to_csv(path_stop+\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i75m9bR9gJdB"
   },
   "source": [
    "# **Example 4: Searching a single author name within a different time-frame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV3IJ1ODwG5y"
   },
   "source": [
    "The search will focus on papers from one single author that were published in a different time-frame. There are two options for doing so:\n",
    "   - A single year (e.g. 2010): in this case the code will query ADS for articles published by the specified authors between one year prior to 4  years after. So searching year='2010' will search articles between 2009 and 2014<br>\n",
    "   - A year range: in this case the syntax is year='[YEAR TO YEAR]' (e.g. year='[2009 TO 2023]') <br>\n",
    "\n",
    "The format for a single author name remains the same as before: **name= 'Last, First'**. <br>\n",
    "\n",
    "Here are two examples:\n",
    "- Searching for Dr. Pepper's articles between year 1999 and 2004\n",
    "- Searching for Dr. Pepper's articles between year 2019 and 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYW3Ng5Emlck"
   },
   "outputs": [],
   "source": [
    "datf=AP.ads_search(name= 'Pepper, Joshua', year='2000',\n",
    "               token=token, stop_dir=stop_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZrvBFHgwG5y"
   },
   "outputs": [],
   "source": [
    "# To display the data frame run the following:\n",
    "datf\n",
    "# To save it in a excel format run the following:\n",
    "#datf.to_csv(path_stop+\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nqcQ42XwG5y"
   },
   "outputs": [],
   "source": [
    "datf=AP.ads_search(name= 'Pepper, Joshua', year='[2019 TO 2023]',\n",
    "               token=token, stop_dir=stop_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XciTTpIn7LQX"
   },
   "outputs": [],
   "source": [
    "# To display the data frame run the following:\n",
    "datf\n",
    "# To save it in a excel format run the following:\n",
    "#datf.to_csv(path_stop+\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLYqtHbfgJhE"
   },
   "source": [
    "# **Example 5: Searching a single institution name within a specific time-frame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO3mJmxzwG5y"
   },
   "source": [
    "The search will focus on authors that publishes as first authors affiliated to a specific institution in a defined timespan. <br>\n",
    "The format for a author name is the same in previous example (**\"Last name, First name\"**) and specified year range is similar to the option provided earlier:<br>\n",
    "   - A single year (e.g. 2010): in this case the code will query ADS for articles published by the specified authors between one year prior to 4  years after. So searching year='2010' will search articles between 2009 and 2014<br>\n",
    "   - A year range: in this case the syntax is year='[YEAR TO YEAR]' (e.g. year='[2009 TO 2023]') <br>\n",
    "\n",
    "Following we present two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Je_jZyFpml8r"
   },
   "outputs": [],
   "source": [
    "datf=AP.ads_search(institution=\"University of Southern Mississippi\",year='2000',\n",
    "               token=token, stop_dir=stop_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhLKT3-jcCSz"
   },
   "outputs": [],
   "source": [
    "# To display the data frame run the following:\n",
    "datf\n",
    "# To save it in a excel format run the following:\n",
    "#datf.to_csv(path_stop+\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93GCVzbigJmG"
   },
   "source": [
    "# **Example 6: Searching a single Author, at a specific institution and within a specific time-frame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2ZfN11BwG5z"
   },
   "source": [
    "The following example combines several of the previous ones in a single search.\n",
    "Specifically:<br>\n",
    "   - A single author<br>\n",
    "   - Affiliated to a single institutions<br>\n",
    "   - In a specific time frame of publications<br>\n",
    "    \n",
    "Please, refer to the previous examples for the sintax required. <br>\n",
    "Here are an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjkE8YGqmmcy"
   },
   "outputs": [],
   "source": [
    "datf=AP.ads_search(name= 'Brown, Beth A.', institution=\"Howard university\",year='[2009 TO 2022]',\n",
    "               token=token, stop_dir=stop_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QK1TJgTwG5z"
   },
   "outputs": [],
   "source": [
    "# To display the data frame run the following:\n",
    "datf\n",
    "# To save it in a excel format run the following:\n",
    "#datf.to_csv(path_stop+\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0iJa3idgJqC"
   },
   "source": [
    "# **Example 7: Searching through a list of institutions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGCvvFuewG50"
   },
   "source": [
    "The search will focus on papers from a list of institutions, so the input is a csv file that has multiple institution names stored in it. This will then find all papers from those institutions (**see CAVEATS in Example 2 above related to Institution Names)**:<br>\n",
    "\n",
    "The input file has to be a .csv file (e.g.\"top10inst.csv\"), and must contain at least one column titled  **\"Current Institution\"** or **\"Institution\"** (the first cell of the column is usually interpreted as such). The file can contains other columns, they will be ignored.<br>\n",
    "If the file is in a different directory than the one where the code it, include the whole path. <br>\n",
    "\n",
    "The code will run as in Example 2 above for each institutions and append the results at each iteration providing a final dataframe with all the researchers at all the institutions in the list provided.<br>\n",
    "**NOTE: at the moment if an institution query returns an empty dataframe the code will ignore it and continue to the following one.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wen24dqrwG50",
    "outputId": "673f5f77-23ba-481f-f04f-358e97928fe9"
   },
   "outputs": [],
   "source": [
    "datf=AP.run_file_search(filename='Fellows_Example.csv',\n",
    "               token=token, stop_dir=stop_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kHud_2hPwG50",
    "outputId": "638370e3-dd06-49a3-ce23-c18330d75bd5"
   },
   "outputs": [],
   "source": [
    "# To display the data frame run the following:\n",
    "datf\n",
    "# To save it in a excel format run the following:\n",
    "#datf.to_csv(path_stop+\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PC0t44PjnokI"
   },
   "source": [
    "# **Example 8: Searching through a list of Authors names**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIWT9esWwG55"
   },
   "source": [
    "The search will focus on papers from a list of authors names (similar format as Example 1 above, **'Last, First'**). <br>\n",
    "The input is a .csv file that has multiple authors names stored in it under a column Title: **\"Name\"**. <br>\n",
    "The ADS search will focus on the period 2003 to 2023.\n",
    "<br>\n",
    "If the file is in a different directory than the one where the code it, include the whole path. <br>\n",
    "\n",
    "The code will then execute the search one name after the other and uppend each result to the previous one.<br>\n",
    "In the following example we use, for convenience, the same example file as before which also contain a list of researchers names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAe9-O_YwG55",
    "outputId": "35c435f2-b521-4950-db6c-ec13fe46fb7e"
   },
   "outputs": [],
   "source": [
    "datf=AP.run_file_search(filename='Fellows_Example.csv',\n",
    "               token=token, stop_dir=stop_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "00bBkTr1wG55",
    "outputId": "30f0d258-b5f9-4d85-8a23-8df733c7a454"
   },
   "outputs": [],
   "source": [
    "# To display the data frame run the following:\n",
    "datf\n",
    "# To save it in a excel format run the following:\n",
    "#datf.to_csv(path_stop+\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 9: Searching a list of Authors at Institutionsd during specific times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9kC4myBBGM0"
   },
   "outputs": [],
   "source": [
    "datf=AP.run_file_search(filename='Fellows_Example.csv',\n",
    "               token=token, stop_dir=stop_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8D4hVQDBN-u"
   },
   "outputs": [],
   "source": [
    "# To display the data frame run the following:\n",
    "datf\n",
    "# To save it in a excel format run the following:\n",
    "#datf.to_csv(path_stop+\"output.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
